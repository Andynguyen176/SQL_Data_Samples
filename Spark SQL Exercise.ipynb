{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Load Data\n\n1. In the following cell, insert a Spark Session DataFrame from the file panel for your user.json.bz2 file which contains the user data from the Yelp Dataset Challenge.  \n\n2. In the cell below that, add your bucket name as the second parameter in the call to `cos.url` to define a URL to the user data file.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 107, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by DSX for sharing."
        }, 
        {
            "execution_count": 108, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "path_user = cos.url('user.json.bz2', 'spring2018andy023363be332e40639c4287c87e0af5e0')"
        }, 
        {
            "execution_count": 109, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "user count: 1326101\nroot\n |-- average_stars: double (nullable = true)\n |-- compliment_cool: long (nullable = true)\n |-- compliment_cute: long (nullable = true)\n |-- compliment_funny: long (nullable = true)\n |-- compliment_hot: long (nullable = true)\n |-- compliment_list: long (nullable = true)\n |-- compliment_more: long (nullable = true)\n |-- compliment_note: long (nullable = true)\n |-- compliment_photos: long (nullable = true)\n |-- compliment_plain: long (nullable = true)\n |-- compliment_profile: long (nullable = true)\n |-- compliment_writer: long (nullable = true)\n |-- cool: long (nullable = true)\n |-- elite: array (nullable = true)\n |    |-- element: long (containsNull = true)\n |-- fans: long (nullable = true)\n |-- friends: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- funny: long (nullable = true)\n |-- name: string (nullable = true)\n |-- review_count: long (nullable = true)\n |-- useful: long (nullable = true)\n |-- user_id: string (nullable = true)\n |-- yelping_since: string (nullable = true)\n\n"
                }
            ], 
            "source": "df_user = spark.read.json(path_user)\nprint \"user count:\", df_user.count()\ndf_user.printSchema()"
        }, 
        {
            "source": "### Create a Temporary View\n\nThe follwoing cell shows creating a temporary view from the `df_user` DataFrame.  There are two ways to create a temporary view.  here we are \nusing the `createOrReplaceTempView` method of the DataFrame.  If you don't remember this long name, as long as your DataFrame has already been created, \njust type the DataFrame's name follwowed by a period and then start typing the first few characters of the command.  At that point, press the tab key and \nJupyter will prompt you with a list of method names to select from.\n\nIn the command for creating the temporary view, the string passed as the parameter is used to name the temporary view.  Whn querrying using Spark SQL, you\nwill use this name as though it were a table.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 110, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": " df_user.createOrReplaceTempView(\"user\")"
        }, 
        {
            "source": "### Load the SSA Gender Data\n\nThis is the ssa_name_gender.tsv file that is based on the Social Security Administration data\nand the wrangling we did previously.  A DataFrame from that was written out to \nObject Storage and downloaded so you could upload it here.\n\n1. In the cell below insert a Spark Session Setup for the file.  Since this is the second file you are loading from that bucket, the boilerplate is not added.\n2. Change the name of the varaible being declared from `path_1` to `path_gender`\n3. When you run the cell after that, it creates a DataFrame based on the URL from your cloud object storage and then creates a temporary view named `gender`.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 111, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\n# Please read the documentation of PySpark to learn more about the possibilities to load data files.\n# PySpark documentation: https://spark.apache.org/docs/2.0.1/api/python/pyspark.sql.html#pyspark.sql.SparkSession\n# The SparkSession object is already initialized for you.\n# The following variable contains the path to your file on your IBM Cloud Object Storage.\npath_2 = cos.url('ssa_name_gender.tsv.bz2', 'spring2018andy023363be332e40639c4287c87e0af5e0')\n"
        }, 
        {
            "execution_count": 112, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "path_gender = cos.url('ssa_name_gender.tsv.bz2', 'spring2018andy023363be332e40639c4287c87e0af5e0')"
        }, 
        {
            "execution_count": 113, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "name count: 96174\nroot\n |-- name: string (nullable = true)\n |-- F: string (nullable = true)\n |-- M: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- gender_ratio: string (nullable = true)\n\n"
                }
            ], 
            "source": "df_gender = spark.read.option(\"header\",\"true\").option(\"sep\",\"\\t\").csv(path_gender)\nprint \"name count:\", df_gender.count()\n\ndf_gender.printSchema()\n\ndf_gender.createOrReplaceTempView(\"gender\")"
        }, 
        {
            "source": "# Basic Queries Against Temporary Views\n\nIn the following cell is the skeleton of a SELECT query against the User table.  The required fields in each query are:\n\n* SELECT (this tells Spark which \"columns\" we want from the data)\n* FROM (this tells Spark what temporary view to get the data from)\n\n\nTake a look at the schema for the df_user DataFrame.  The fields we want to include are the user_id, name, and colums for the userful, funny, and cool votes that user has cast.\nWhat is the name of your temporary view for users?  That goes in the FROM clause.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 114, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "SQL SELECT QUERY: \n+----------------------+-------+------+-----+----+\n|Users                 |name   |useful|funny|cool|\n+----------------------+-------+------+-----+----+\n|oMy_rEb0UBEmMlu-zcxnoQ|Johnny |0     |0    |0   |\n|JJ-aSuM4pCFPdkfoZ34q0Q|Chris  |0     |0    |0   |\n|uUzsFQn_6cXDh6rPNGbIFA|Tiffy  |0     |0    |0   |\n|mBneaEEH5EMyxaVyqS-72A|Mark   |0     |0    |0   |\n|W5mJGs-dcDWRGEhAzUYtoA|Evelyn |0     |0    |0   |\n|4E8--zUZO1Rr1IBK4_83fg|Lisa   |4     |0    |0   |\n|Ob-2oGBQ7rwwYwUvhmnf7g|B      |0     |0    |0   |\n|JaTVvKsBl0bHHJEpESn4pQ|Peter  |0     |0    |0   |\n|Ykj0DVsz0c6rX9ghjd0hDg|Colleen|0     |0    |0   |\n|kmyEPfKnHQJdTceCdoyMQg|A      |0     |0    |0   |\n|H54pA7YHfjl8IjhHAfdXJA|Chad   |0     |0    |0   |\n|WRae-wZkpRoxMrgJdqwyxg|Mike   |0     |0    |0   |\n|Mmv5fPxbF8XEMN4EPT_Khg|Chris  |0     |0    |0   |\n|LdqGHXsNQowMrvgTNburJA|Susan  |3     |1    |0   |\n|TsgBsn19Wjwpyo81gF9_8Q|Cathy  |0     |0    |0   |\n|V--GjQPlTpeWbcB2cS06Gw|Cody   |2     |0    |1   |\n|a_gKYQ5YMg39FHNYJLWRHg|Joselyn|0     |0    |0   |\n|sz8Heh56kO_6_LBkoIhNfA|Rosalie|0     |0    |0   |\n|h5ERTYn2vQ1QbjTZvfWPaA|Bobby  |0     |0    |0   |\n|jYnkJR3T8yCERXywoVhWYA|Hugo   |15    |6    |2   |\n+----------------------+-------+------+-----+----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "print \"SQL SELECT QUERY: \"\n\nspark.sql(\"\"\"\nSELECT user_id as Users, name, useful, funny, cool\nFROM user\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "# Adding Conditions with a WHERE clause\n\nthe SELECT and FROM as used above will return every row in the temporary view.  This is similar to the `filter` method for DataFrames. If we want only certain rows, \nwe need to add a WHERE clause to the query.  In the following cell, run the same query, but this time, get only\nthose users who have written over 50 reviews:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 115, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "SQL WHERE CLAUSE: \n+----------------------+---------+------------+-------+\n|Users                 |Name     |Yelper_Since|Reviews|\n+----------------------+---------+------------+-------+\n|bZkZgll3Fii18x3WRtB5Lg|Dale     |2010-01-06  |62     |\n|aw973Pm1nrTbRjP4zY9B9g|Kenny    |2008-09-23  |762    |\n|B46q0uJGiuzTtcYXknkgcQ|izagui   |2008-01-14  |83     |\n|oMT1lSrACglwgFPDBYEIuA|mmaaxxm\u00fcl|2011-02-06  |54     |\n|X-yLZasrQYb4hWTdP-fOGQ|K And A  |2013-10-14  |54     |\n|m-Pnm53eK4NFW6keiL7yjQ|Donny    |2010-07-17  |149    |\n|ioT4UHRwWlbAidIJHfnvOg|Randy    |2006-11-06  |154    |\n|wm97KC6G0resSDXTmNIMKw|Dwain    |2012-06-08  |1262   |\n|i5jSTSpXJtvM-ExWRttglw|Evelina  |2014-03-17  |85     |\n|dQ5yKW2B1M-EvS1BqKfxWg|David    |2012-10-18  |109    |\n|snDjs1hdh7JOWv4jjbXPDw|Michael  |2008-11-19  |447    |\n|NSszc7yDLIlt1tzINtGNRg|Ryan     |2011-06-22  |200    |\n|hgLpWCiE3tWvBYfP0q2wLg|jim      |2010-01-14  |54     |\n|r-00EZGnVEHCRH7IkVDN1Q|bef      |2008-07-25  |59     |\n|MiDcQ-bgIg4B91reFV4Qaw|Carly    |2012-08-22  |110    |\n|IkoXyeNTHqpU24dHKmIcrw|Kurt     |2006-05-12  |54     |\n|bBRPy8zUvNc0NGbGmkjrZg|Jan      |2009-03-30  |462    |\n|y81JS4QQOFByWrbJLBWO9g|Eric     |2009-08-28  |76     |\n|37Hc8hr3cw0iHLoPzLK6Ow|Christine|2008-03-03  |496    |\n|MgAHuFGzScMAuLpDmHK7iQ|Martin   |2012-04-09  |88     |\n+----------------------+---------+------------+-------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "print \"SQL WHERE CLAUSE: \"\n\nspark.sql(\"\"\"\nSELECT user_id as Users, name as Name, yelping_since as Yelper_Since, review_count as Reviews\nFROM user\nWHERE review_count > 50\n\"\"\").show(truncate=False)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# This somewhat ugly cell in the notebook is to align the table in the next cell\n%%html\n<style>\ntable {float:left}\n</style>"
        }, 
        {
            "source": "### Comparisons in the WHERE Clause\nMost of the comparisons you will be doing you learned in grade school.  \nFollowing is a table of the possible comparisons you can do.  \n\n\n| Operator | What it Does | Example |\n| --- | --- | --- |\n|  <       | less than | review_count < 100 |\n| <=       | less than or equal to | review_count <= 100 |\n| >        | greater than | review_count > 100 |\n| >=        | greater than or equal to | review_count >= 100 |\n| =        | equal | review_count = 100 |\n| <>        | not equal to | review_count <> 100 |\n| +        | add two fields | funny + cool > 100 | \n| -        | subtract two fields | funny - cool > 100 |\n| *        | multiply two fields | funny * 2 > useful |\n|  /       | divide one field by another<br/> (The divisor on the bottom cannot be zero) |funny / 2 > useful |\n| MOD      | remainder of dividing one number by another | funny MOD 10 > 5<br/><br/> Means that if funny is divided by 10, <br/>the remainder is greater than 5, so a <br/>funny value of 16 matches (16 / 10 = 6) |\n| BETWEEN *x and *y | matches values in a range <br/>(including the end points)  | funny BETWEEN 5 AND 20 |\n| NOT      | finds rows that don't match the condition | funny NOT BETWEEN 5 and 20 <br/>matches rows where funny is less than 5 or more than 20 |\n| IS NULL  | matches rows where the specified field is null | funny IS NULL |\n| IS NOT NULL | this is an example of combining `NOT` and `IS NULL` | matches all rows where funny is not NULL |", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Adding Multiple Conditions in the WHERE Clause\nYou can have multiple conditions in the WHERE clause. If you want to combine multiple conditions, use `AND` and `OR` to combine \nthe conditions and use parenthesis to set the order of precedence for how the ANDs and ORs are considered (your junior high\nschool math teacher said that would be useful some day).\n\nIn the next cell, create a query that gets those users who have written over 50 reviews and have voted cool more than 20 times:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 119, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------------+---------+------------+-------+----+\n|Users                 |Name     |Yelper_Since|Reviews|Cool|\n+----------------------+---------+------------+-------+----+\n|aw973Pm1nrTbRjP4zY9B9g|Kenny    |2008-09-23  |762    |47  |\n|wm97KC6G0resSDXTmNIMKw|Dwain    |2012-06-08  |1262   |94  |\n|snDjs1hdh7JOWv4jjbXPDw|Michael  |2008-11-19  |447    |22  |\n|MiDcQ-bgIg4B91reFV4Qaw|Carly    |2012-08-22  |110    |172 |\n|bBRPy8zUvNc0NGbGmkjrZg|Jan      |2009-03-30  |462    |764 |\n|37Hc8hr3cw0iHLoPzLK6Ow|Christine|2008-03-03  |496    |310 |\n|oH9K7eCuNsYr6MmlM2ZjUg|Buo      |2007-11-10  |902    |42  |\n|z9MozWK9f7C8p3Gj0uOiHw|Marna    |2009-10-17  |246    |105 |\n|bzMzZE3OCqHhZyXH5JRaWw|Lucy     |2008-09-29  |851    |169 |\n|qSh-q8M-rL4PRVukXsDwWg|Ellen    |2007-12-28  |228    |30  |\n|lmJy4OwP_TyHIg8a8Q0RsA|Alan     |2013-06-14  |646    |73  |\n|keLUgL_4y60BkppiAsIk8Q|Hazel    |2014-06-21  |229    |54  |\n|et_GDGFfG2BFVkLzRK2mTQ|Linda    |2008-07-31  |269    |42  |\n|KBVL9aPlcLVwqyFQ__EeIA|Kenny    |2011-09-19  |91     |24  |\n|6ZR7HuUpHk_NQNs0dT0oLg|Beverly  |2007-03-12  |388    |169 |\n|mqzisCyZ_tlpQt6HkNnhOw|Karen    |2009-04-10  |388    |50  |\n|1D5EQITb4hOzPMZT8gDPQQ|Krista   |2008-12-23  |324    |35  |\n|sfBfh6X2v2RDSz49DUVMxw|Mary Jane|2006-12-22  |458    |35  |\n|PIM9LfrgvCqOoOPBdJAcNg|Brian    |2009-11-28  |110    |39  |\n|31d4d1tVHxT_8LfPvGlzYQ|Nic      |2009-07-15  |443    |33  |\n+----------------------+---------+------------+-------+----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT user_id as Users, name as Name, yelping_since as Yelper_Since, review_count as Reviews, \n        compliment_cool as Cool\nFROM user\nWHERE review_count > 50 AND compliment_cool > 20\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "### Ordering multiple conditions\n\nIn the follwing cell, modify your query you have been doing so that you get those reviewers who have written over 50 reviews and have either:\n* Cast between 10 and 20 funny votes\n* Cast 20 or more cool votes", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 120, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Multiple Conditions: \n+----------------------+---------+------------+-------+-----+----+\n|Users                 |Name     |Yelper_Since|Reviews|Funny|Cool|\n+----------------------+---------+------------+-------+-----+----+\n|aw973Pm1nrTbRjP4zY9B9g|Kenny    |2008-09-23  |762    |47   |47  |\n|wm97KC6G0resSDXTmNIMKw|Dwain    |2012-06-08  |1262   |94   |94  |\n|i5jSTSpXJtvM-ExWRttglw|Evelina  |2014-03-17  |85     |13   |13  |\n|snDjs1hdh7JOWv4jjbXPDw|Michael  |2008-11-19  |447    |22   |22  |\n|MiDcQ-bgIg4B91reFV4Qaw|Carly    |2012-08-22  |110    |172  |172 |\n|bBRPy8zUvNc0NGbGmkjrZg|Jan      |2009-03-30  |462    |764  |764 |\n|37Hc8hr3cw0iHLoPzLK6Ow|Christine|2008-03-03  |496    |310  |310 |\n|oH9K7eCuNsYr6MmlM2ZjUg|Buo      |2007-11-10  |902    |42   |42  |\n|z9MozWK9f7C8p3Gj0uOiHw|Marna    |2009-10-17  |246    |105  |105 |\n|bzMzZE3OCqHhZyXH5JRaWw|Lucy     |2008-09-29  |851    |169  |169 |\n|qSh-q8M-rL4PRVukXsDwWg|Ellen    |2007-12-28  |228    |30   |30  |\n|cT5d9cgC3It82XTlOUCH5w|Alicia   |2011-12-26  |118    |13   |13  |\n|lmJy4OwP_TyHIg8a8Q0RsA|Alan     |2013-06-14  |646    |73   |73  |\n|keLUgL_4y60BkppiAsIk8Q|Hazel    |2014-06-21  |229    |54   |54  |\n|yHbrIShhhHkkl20_KGP4OA|Charlie  |2010-11-07  |380    |10   |10  |\n|et_GDGFfG2BFVkLzRK2mTQ|Linda    |2008-07-31  |269    |42   |42  |\n|KBVL9aPlcLVwqyFQ__EeIA|Kenny    |2011-09-19  |91     |24   |24  |\n|6ZR7HuUpHk_NQNs0dT0oLg|Beverly  |2007-03-12  |388    |169  |169 |\n|mqzisCyZ_tlpQt6HkNnhOw|Karen    |2009-04-10  |388    |50   |50  |\n|1D5EQITb4hOzPMZT8gDPQQ|Krista   |2008-12-23  |324    |35   |35  |\n+----------------------+---------+------------+-------+-----+----+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "print \"Multiple Conditions: \"\n\nspark.sql(\"\"\"\nSELECT user_id as Users, name as Name, yelping_since as Yelper_Since, review_count as Reviews, \n        compliment_funny as Funny, compliment_cool as Cool\nFROM user\nWHERE review_count > 50 \nAND compliment_funny >= 20 \nOR compliment_cool BETWEEN 10 AND 20\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "# Aggregation, Grouping and the GROUP BY Clause\n\nThe next clause to add to our query is aggregation.  So far we have been selecting individual rows and the results have not involved any interaction between rows in the data.\n\nWhat if we wanted to the the total number of reviews that have been written by the users in our dataset and the total number of cool votes they have cast?  That's what the following query does (run it).\n\nWhy are we aliasing the calculated sums? `SUM(review_count) AS reviews`  What happens if you do not as `AS reviews`? Try it out.\n\nIn the following query we are using SUM, but there are other aggregation functions you can use.  Some of the more common aggregation functions are listed in the followng table.  The \"what's returned\" column assumes you are not doing any grouping.\n\nRun the query\n\n| Function | Example | What's Returned |\n| ----- | ---- | --- |\n| SUM | SUM(cool) | total of the cool votes cast |\n| AVG | AVG(review_count) | the average number of reviews <br/>written across all of the users |\n|STDDEV | STDDEV(review_count) | the standard deviation of the <br/>review_count across all of the users |\n| MIN | MIN(review_count) | the lowest review count <br/>of any user in the data |\n| MAX | MAX(review_count) | the most reviews any user <br/>in the data has written |\n| COUNT | COUNT(user_id) | counts the number of users in the data<br/>since we can get a count of the DataFrame, <br/>this is more useful when grouping |\n| DISTINCT | COUNT (DISTINCT name) | DISTINCT is not a grouping function, but is <br/>often used with grouping - it excludes<br/>duplicates from the result. |", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 121, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------+----------+\n|reviews |cool_votes|\n+--------+----------+\n|30655691|25996163  |\n+--------+----------+\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT SUM(review_count) AS reviews, SUM(cool) AS cool_votes\nFROM user\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "### Adding a GROUP BY clause\n\nThe above query returns a single row with totals for all of our users, but what if we wanted to group our users?\n\nFor example, we have the average star rating for each user (across all of their reviews), so what if we wanted to group them into 5 groups 1 - 5 based on rounding their average stars and then calculate:\n* The number of users in each group\n* The average number and standard deviation of reviews written by the users in each group\n* The average number of useful votes written by each user in each group\n\nWe could write the following query:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 122, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+------+------------------+------------------+------------------+\n|stars|users |reviews           |rev_stddev        |useful_votes      |\n+-----+------+------------------+------------------+------------------+\n|5.0  |380756|5.3597448234564915|13.525511145777797|3.3632299950624547|\n|4.0  |499538|42.75221905040257 |110.99116669141631|62.74042615376608 |\n|3.0  |265617|24.56398122108148 |79.52535220373554 |21.700990523949898|\n|2.0  |86226 |6.6548488854869765|12.35533766805391 |5.141662607566164 |\n|1.0  |93964 |1.70433357456047  |2.5777271211025155|0.9024945723894258|\n+-----+------+------------------+------------------+------------------+\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT ROUND(average_stars,0) AS stars,\n       COUNT(user_id) AS users,\n       AVG(review_count) AS reviews, \n       STDDEV(review_count) AS rev_stddev,\n       AVG(useful) AS useful_votes\nFROM user\nGROUP BY ROUND(average_stars,0)\nORDER BY stars DESC\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "# Handy Math Functions \n\nIf you have used Excel, the above round function, which is rouding the stars to zero decimal places should look familiar.  The following table lists some other math functions you are likely to find handy in your data wrangling (there are many more).\n\n| Function |  Example | What it Does |\n| -------- | -------- | ------------ |\n| ROUND(x,y) | ROUND(average_stars,0)| Rounds the field named as the first parameter <br/>to the number of decimal places in <br/>the second parameter | \n| CEILING(x) | CEILING(average_stars)| Rounds up to the next integer value, <br/>so if the average_stars <br/>for a user was 3.4 or 3.8, the <br/>example shown would return 4 for <br/>that user |\n| FLOOR(x) | FLOOR(average_stars)| Rounds down to the next integer value, <br/>so if the average_stars <br/>for a user was 3.4 or 3.8, the <br/>example shown would return 3 for <br/>that user |", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Sorting the Results\n\nTo see the results of the above query in sorted order (descending order based on the `stars` field) add the following line after the GROUP BY clause and rerun the query:\n\n`\nORDER BY stars DESC\n`\n\nNote that in the ORDER BY clause we can use the alias for the `stars` field instead of the formula.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Filtering the Aggregated Results\n\nEarlier we used the WHERE clause to filter the rows included in your query's result, once you have aggregated, you can filter again using a HAVING clause.  Copy your query from above to the following blank cell and then add the following line between your GROUP BY and ORDER BY clauses:\n\n`\nHAVING users >= 100000\n`\n\nThis tells Spark to filter the result so that only the groups that have at least 100,000 users should be included in the result.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 123, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+------+------------------+------------------+------------------+\n|stars|users |reviews           |rev_stddev        |useful_votes      |\n+-----+------+------------------+------------------+------------------+\n|5.0  |380756|5.3597448234564915|13.525511145777797|3.3632299950624547|\n|4.0  |499538|42.75221905040257 |110.99116669141631|62.74042615376608 |\n|3.0  |265617|24.56398122108148 |79.52535220373554 |21.700990523949898|\n+-----+------+------------------+------------------+------------------+\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT ROUND(average_stars,0) AS stars,\n       COUNT(user_id) AS users,\n       AVG(review_count) AS reviews, \n       STDDEV(review_count) AS rev_stddev,\n       AVG(useful) AS useful_votes\nFROM user\nGROUP BY ROUND(average_stars,0)\nHAVING users >= 100000\nORDER BY stars DESC\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "# JOIN to Bring Together Multiple Temporary Views\n\nThe JOIN matches rows between two (or more) views based on whether the field(s) specified in the join match.\n\nWe will explore three types of joins you might use:\n\n|Type of Join | What the Result Includes |\n| ----------- | ------------------------ |\n| INNER JOIN | Includes rows from both tables only if the matching field value is in both views  |\n| LEFT OUTER JOIN | Includes all of the rows from the view on the left in the JOIN <br/>( e.g., viewA INNER JOIN viewB would have ViewA as the view on the left) |\n| FULL OUTER JOIN | Includes each row in each table, regardless of whether it's in the other table |", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "In the following cell we are doing an inner join of the user and gender views based on the name fields.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 124, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------------+-------+------------+-------------+------+------------------+\n|user_id               |name   |review_count|average_stars|gender|gender_ratio      |\n+----------------------+-------+------------+-------------+------+------------------+\n|oMy_rEb0UBEmMlu-zcxnoQ|Johnny |8           |4.67         |2     |0.988185431137896 |\n|JJ-aSuM4pCFPdkfoZ34q0Q|Chris  |10          |3.7          |2     |0.8625031523579637|\n|uUzsFQn_6cXDh6rPNGbIFA|Tiffy  |1           |2.0          |1     |1.0               |\n|mBneaEEH5EMyxaVyqS-72A|Mark   |6           |4.67         |2     |0.9966800205261983|\n|W5mJGs-dcDWRGEhAzUYtoA|Evelyn |3           |4.67         |1     |0.9966969953487197|\n|4E8--zUZO1Rr1IBK4_83fg|Lisa   |11          |3.45         |1     |0.9971182300888615|\n|JaTVvKsBl0bHHJEpESn4pQ|Peter  |2           |5.0          |2     |0.9966299930617504|\n|Ykj0DVsz0c6rX9ghjd0hDg|Colleen|1           |1.0          |1     |0.9979678927047348|\n|H54pA7YHfjl8IjhHAfdXJA|Chad   |3           |5.0          |2     |0.9956792048738902|\n|WRae-wZkpRoxMrgJdqwyxg|Mike   |1           |5.0          |2     |0.9967678621996113|\n|Mmv5fPxbF8XEMN4EPT_Khg|Chris  |2           |1.67         |2     |0.8625031523579637|\n|LdqGHXsNQowMrvgTNburJA|Susan  |1           |2.0          |1     |0.9977404739932028|\n|TsgBsn19Wjwpyo81gF9_8Q|Cathy  |4           |4.0          |1     |0.9979871955119741|\n|V--GjQPlTpeWbcB2cS06Gw|Cody   |2           |3.0          |2     |0.9827712635570671|\n|a_gKYQ5YMg39FHNYJLWRHg|Joselyn|1           |5.0          |1     |0.9996917005796029|\n|sz8Heh56kO_6_LBkoIhNfA|Rosalie|4           |3.0          |1     |0.9986268255353234|\n|h5ERTYn2vQ1QbjTZvfWPaA|Bobby  |3           |3.33         |2     |0.9701818790656142|\n|jYnkJR3T8yCERXywoVhWYA|Hugo   |48          |3.73         |2     |0.9990235328581193|\n|eMBV7FugCJq7FIvGhARo2Q|Jack   |1           |5.0          |2     |0.9961176912261588|\n|h5_D5TlEN4bREoy3vR-Vxw|John   |1           |1.0          |2     |0.9957743547977855|\n+----------------------+-------+------------+-------------+------+------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT U.user_id, U.name, U.review_count, U.average_stars,\n       G.gender, G.gender_ratio\nFROM user AS U INNER JOIN gender AS G\nON LOWER(U.name) = LOWER(G.name)\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "### LEFT OUTER JOIN\n\nCopy the above query to the cell below and modify it to do a LEFT OUTER JOIN", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 125, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------------+-------+------------+-------------+------+------------------+\n|user_id               |name   |review_count|average_stars|gender|gender_ratio      |\n+----------------------+-------+------------+-------------+------+------------------+\n|oMy_rEb0UBEmMlu-zcxnoQ|Johnny |8           |4.67         |2     |0.988185431137896 |\n|JJ-aSuM4pCFPdkfoZ34q0Q|Chris  |10          |3.7          |2     |0.8625031523579637|\n|uUzsFQn_6cXDh6rPNGbIFA|Tiffy  |1           |2.0          |1     |1.0               |\n|mBneaEEH5EMyxaVyqS-72A|Mark   |6           |4.67         |2     |0.9966800205261983|\n|W5mJGs-dcDWRGEhAzUYtoA|Evelyn |3           |4.67         |1     |0.9966969953487197|\n|4E8--zUZO1Rr1IBK4_83fg|Lisa   |11          |3.45         |1     |0.9971182300888615|\n|Ob-2oGBQ7rwwYwUvhmnf7g|B      |9           |4.78         |null  |null              |\n|JaTVvKsBl0bHHJEpESn4pQ|Peter  |2           |5.0          |2     |0.9966299930617504|\n|Ykj0DVsz0c6rX9ghjd0hDg|Colleen|1           |1.0          |1     |0.9979678927047348|\n|kmyEPfKnHQJdTceCdoyMQg|A      |7           |4.29         |null  |null              |\n|H54pA7YHfjl8IjhHAfdXJA|Chad   |3           |5.0          |2     |0.9956792048738902|\n|WRae-wZkpRoxMrgJdqwyxg|Mike   |1           |5.0          |2     |0.9967678621996113|\n|Mmv5fPxbF8XEMN4EPT_Khg|Chris  |2           |1.67         |2     |0.8625031523579637|\n|LdqGHXsNQowMrvgTNburJA|Susan  |1           |2.0          |1     |0.9977404739932028|\n|TsgBsn19Wjwpyo81gF9_8Q|Cathy  |4           |4.0          |1     |0.9979871955119741|\n|V--GjQPlTpeWbcB2cS06Gw|Cody   |2           |3.0          |2     |0.9827712635570671|\n|a_gKYQ5YMg39FHNYJLWRHg|Joselyn|1           |5.0          |1     |0.9996917005796029|\n|sz8Heh56kO_6_LBkoIhNfA|Rosalie|4           |3.0          |1     |0.9986268255353234|\n|h5ERTYn2vQ1QbjTZvfWPaA|Bobby  |3           |3.33         |2     |0.9701818790656142|\n|jYnkJR3T8yCERXywoVhWYA|Hugo   |48          |3.73         |2     |0.9990235328581193|\n+----------------------+-------+------------+-------------+------+------------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT U.user_id, U.name, U.review_count, U.average_stars,\n       G.gender, G.gender_ratio\nFROM user AS U LEFT OUTER JOIN gender AS G\nON LOWER(U.name) = LOWER(G.name)\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "### FULL OUTER JOIN\n\nCopy the above query to the cell below and modify it to do a FULL OUTER JOIN", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 126, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------------+---------------+------------+-------------+------+------------+\n|user_id               |name           |review_count|average_stars|gender|gender_ratio|\n+----------------------+---------------+------------+-------------+------+------------+\n|8yuMP766uRPe6sEcxS1AXQ|'Nikki         |5           |4.14         |null  |null        |\n|MS_IRLwpy-TUGNyUIDRq-Q|1Southerngyrl76|1           |1.0          |null  |null        |\n|k7aF28JCVxi14O4qO_2fEw|A-dogg         |46          |4.0          |null  |null        |\n|null                  |null           |null        |null         |2     |1.0         |\n|WwsMmD9R4Oz0ZqmsY4VxTQ|Advice4u       |4           |4.0          |null  |null        |\n|Zu7gGW6dlGIf3iDG929_oQ|Adwinnie       |40          |3.85         |null  |null        |\n|qMubZnWM_lLRXqYwEbbG9g|Agustinus      |20          |4.14         |null  |null        |\n|null                  |null           |null        |null         |1     |1.0         |\n|null                  |null           |null        |null         |1     |1.0         |\n|null                  |null           |null        |null         |1     |1.0         |\n|null                  |null           |null        |null         |1     |1.0         |\n|pivG5Kp8WdoCekuM6bAjGw|Ailis          |6           |4.33         |1     |1.0         |\n|hJPPMAaczO7VgE-sCqNacQ|Ailis          |28          |4.43         |1     |1.0         |\n|CiK2bpgH8G35aofeolyVBw|Air Rick       |51          |3.64         |null  |null        |\n|vGAxJowYHCWrmYGDwfDcOQ|Akeem          |8           |4.63         |2     |1.0         |\n|m4eHGySVM2lL-SfjuRwDyQ|Akeem          |22          |3.74         |2     |1.0         |\n|r_yzcbcHGOgtmGKRCanZYA|Akeem          |5           |3.8          |2     |1.0         |\n|UHH66JQXFHyXJVgOgml_WA|Akeem          |1           |4.0          |2     |1.0         |\n|YDwKuB3c5T0WqKokNdAzbQ|Akeem          |3           |5.0          |2     |1.0         |\n|uyr9yZ-k8JQo_PVm6h99dw|Akeem          |2           |4.0          |2     |1.0         |\n+----------------------+---------------+------------+-------------+------+------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "spark.sql(\"\"\"\nSELECT U.user_id, U.name, U.review_count, U.average_stars,\n       G.gender, G.gender_ratio\nFROM user AS U FULL OUTER JOIN gender AS G\nON LOWER(U.name) = LOWER(G.name)\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "### Using the IF function to Check for Null\n\nSomething weird happens with the FULL OUTER JOIN.  We have a name for those names in the gender data but that are not in the user data, but that name is not in our result because our SELECT clause\nincludes `U.name`.  What we really want is to get the name from the user view (with the alias \"U\") when there is a user with that name, but if not, then we want the name from the gender view.\n\nThe solution is to use the IF function in SQL instead of `u.name`.  The format of the IF function is as follows:\n\n`\nIF(<some condition>, <result if true>, <result if false>)\n`    \n \nCopy the FULL OUTER JOIN query to the cell below and use the IF function to fix this problem with your query.  If you are not sure how, take a look back at the table of operators listed earlier in this notebook.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 127, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "IF function to check for NULL: \n+----------------------+---------------+------------+-------------+------+------------+\n|user_id               |Name           |review_count|average_stars|gender|gender_ratio|\n+----------------------+---------------+------------+-------------+------+------------+\n|8yuMP766uRPe6sEcxS1AXQ|'Nikki         |5           |4.14         |null  |null        |\n|MS_IRLwpy-TUGNyUIDRq-Q|1Southerngyrl76|1           |1.0          |null  |null        |\n|k7aF28JCVxi14O4qO_2fEw|A-dogg         |46          |4.0          |null  |null        |\n|null                  |acheron        |null        |null         |2     |1.0         |\n|WwsMmD9R4Oz0ZqmsY4VxTQ|Advice4u       |4           |4.0          |null  |null        |\n|Zu7gGW6dlGIf3iDG929_oQ|Adwinnie       |40          |3.85         |null  |null        |\n|qMubZnWM_lLRXqYwEbbG9g|Agustinus      |20          |4.14         |null  |null        |\n|null                  |ahaana         |null        |null         |1     |1.0         |\n|null                  |ahzaria        |null        |null         |1     |1.0         |\n|null                  |ailaina        |null        |null         |1     |1.0         |\n|null                  |aileana        |null        |null         |1     |1.0         |\n|hJPPMAaczO7VgE-sCqNacQ|Ailis          |28          |4.43         |1     |1.0         |\n|pivG5Kp8WdoCekuM6bAjGw|Ailis          |6           |4.33         |1     |1.0         |\n|CiK2bpgH8G35aofeolyVBw|Air Rick       |51          |3.64         |null  |null        |\n|Ld0HoEvt1Pd3quL7A1IR1w|Akeem          |1           |5.0          |2     |1.0         |\n|Al8Qo08M8K2Rq70Gk6xIvQ|Akeem          |2           |5.0          |2     |1.0         |\n|vGAxJowYHCWrmYGDwfDcOQ|Akeem          |8           |4.63         |2     |1.0         |\n|m4eHGySVM2lL-SfjuRwDyQ|Akeem          |22          |3.74         |2     |1.0         |\n|r_yzcbcHGOgtmGKRCanZYA|Akeem          |5           |3.8          |2     |1.0         |\n|UHH66JQXFHyXJVgOgml_WA|Akeem          |1           |4.0          |2     |1.0         |\n+----------------------+---------------+------------+-------------+------+------------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "print \"IF function to check for NULL: \"\n\nspark.sql(\"\"\"\n\nSELECT U.user_id, IF (U.name IS NULL, G.name, U.name) as Name, U.review_count, U.average_stars,\n       G.gender, G.gender_ratio\nFROM user AS U FULL OUTER JOIN gender AS G\nON LOWER(U.name) = LOWER(G.name)\n\"\"\").show(truncate=False)"
        }, 
        {
            "source": "# You're Done With This Exercise\n\nBe sure to do the following:\n* Save your notebook and then save a version of your notebook (from the File menu)\n* Create a sharable link (without sensitive cells)\n* Upload your link as the deliverable for the exercise\n* Test your link - is it showing what you want to submit?\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}